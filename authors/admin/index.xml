<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jorge F. Silva</title>
    <link>https://jorgesil77.github.io/authors/admin/</link>
    <description>Recent content on Jorge F. Silva</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Jorge F. Silva 2019</copyright>
    <lastBuildDate>Wed, 01 Aug 2018 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://jorgesil77.github.io/authors/admin/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Lossy Source Coding and Model Identification: Connection with Zero-Rate Density Estimation and the Skeleton Estimator</title>
      <link>https://jorgesil77.github.io/publication/usc-model/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://jorgesil77.github.io/publication/usc-model/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Compressibility Analysis of Asymptotically Mean Stationary Processes</title>
      <link>https://jorgesil77.github.io/publication/compresibility-ams/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://jorgesil77.github.io/publication/compresibility-ams/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Shannon Entropy Estimation in infinite alphabets</title>
      <link>https://jorgesil77.github.io/publication/entropy-infinity/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://jorgesil77.github.io/publication/entropy-infinity/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Universal Weak Variable-Length Source Coding on Countably Infinite Alphabets</title>
      <link>https://jorgesil77.github.io/publication/weak-usc-infinity/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://jorgesil77.github.io/publication/weak-usc-infinity/</guid>
      <description></description>
    </item>
    
    <item>
      <title>On the Characterization of Compressible Ergodic Sequences</title>
      <link>https://jorgesil77.github.io/publication/compresibility-ergodic/</link>
      <pubDate>Mon, 01 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://jorgesil77.github.io/publication/compresibility-ergodic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Complexity-Regularized Tree-Structured Partition for Mutual Information Estimation</title>
      <link>https://jorgesil77.github.io/publication/tsp-mi-est/</link>
      <pubDate>Thu, 01 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>https://jorgesil77.github.io/publication/tsp-mi-est/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://jorgesil77.github.io/authors/admin/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://jorgesil77.github.io/authors/admin/</guid>
      <description>&lt;p&gt;I am faculty working at the Information and Decision System group,  Universidad de Chile, Santiago, Chile. Throughout my research career, I have been passionate about the interaction between learning and information theory. Some of my recent works have been on: universal source coding (lossy and lossless) in $\infty$-alphabets, the estimation of information measures (entropy, mutual information and divergence), learning and decision with data-rate constraints, and the characterization of nearly sparse ($\ell_p$-compressible) random sequences when relaxing stationary and ergodic assumptions.&lt;/p&gt;

&lt;p&gt;I am currently interested in analyzing some connections between learning and information theory;  learning and universal source coding;  sparsity, invariance and other statistical structures that can be learned-detected from the data; as well as understanding the generalization capacity of complex learning algorithms.&lt;/p&gt;

&lt;p&gt;I love music (the aspects of blues and jazz improvisation) and playing electric guitar.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
